{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjoTs2tbc_-a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import requests\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#hyperparameters\n",
        "n_layer = 4\n",
        "n_embedding = 192\n",
        "n_head = 3\n",
        "batch_size = 32\n",
        "block_size = 128 # what is the maximum context length for predictions?\n",
        "learning_rate = 1e-3\n",
        "dropout_rate = 0.1\n",
        "num_epochs = 10\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text"
      ],
      "metadata": {
        "id": "zlnHCmgg8oGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building vocabulary (character-level for simplicity)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)} #Maps each character to a unique integer index\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encode/decode functions\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "h32lpw2Z8qbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.block_size]\n",
        "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
        "        return x, y\n",
        "\n",
        "# Prepare data\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))  # 90% train, 10% val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "6Gwxwol1EFpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(train_data, block_size)\n",
        "val_dataset = TextDataset(val_data, block_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "AatXp73N-AnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Model architecture from scratch"
      ],
      "metadata": {
        "id": "LvGOTh6b-IYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"one head of self attention\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embedding,head_size,bias=False)\n",
        "    self.query = nn.Linear(n_embedding,head_size,bias=False)\n",
        "    self.value = nn.Linear(n_embedding,head_size,bias=False)\n",
        "    self.tril = torch.tril(torch.ones(block_size, block_size)).to(device)  # Lower triangular matrix for masking\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      B,T,C = x.shape\n",
        "      k = self.key(x)\n",
        "      q = self.query(x)\n",
        "      v = self.value(x)\n",
        "\n",
        "      \"compute attention\"\n",
        "      w = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "      w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "      w = F.softmax(w,dim=-1)\n",
        "\n",
        "      v = self.value(x)\n",
        "      out = w @ v\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "OUsy6eQ-eSPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"multiple heads of attention in parallel\"\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embedding,n_embedding)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads],dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "ZnGBaAfOjix5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"a linear project layer\"\n",
        "  def __init__(self,n_embeddings):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embeddings,4*n_embeddings),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embeddings,n_embeddings),\n",
        "        nn.Dropout(dropout_rate)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.net(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "roJPGeOUkm_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, n_embedding, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embedding // n_head\n",
        "    self.sa = MultiHeadAttention(n_head,head_size)\n",
        "    self.ffwd = FeedForward(n_embedding)\n",
        "    self.ln1 = nn.LayerNorm(n_embedding)\n",
        "    self.ln2 = nn.LayerNorm(n_embedding)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.sa(self.ln1(x))   # includes the risidual connections\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "cubgwvrWlL_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size,n_embedding)\n",
        "    self.position_embedding = nn.Embedding(block_size,n_embedding)\n",
        "    self.blocks = nn.Sequential(*[TransformerBlock(n_embedding, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embedding)\n",
        "    self.lm_head = nn.Linear(n_embedding,vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self,x,targets=None):\n",
        "    B, T = x.shape\n",
        "\n",
        "    token_emb = self.token_embedding(x)\n",
        "    pos_emb = self.position_embedding(torch.arange(T,device=device))\n",
        "    x = token_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits= self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B * T, C)\n",
        "      targets = targets.view(B * T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "s1P5dlywnMFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = GPTLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "07DVjwgHBDV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  num_batches = 0\n",
        "  for xb, yb in train_loader:\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    num_batches += 1\n",
        "  avg_loss = total_loss / num_batches\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8XBPKheBKd8",
        "outputId": "2ee7d4b8-65ee-4e84-c102-7d2e4eb3411a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Average Loss: 1.1867\n",
            "Epoch 2/10, Average Loss: 0.9786\n",
            "Epoch 3/10, Average Loss: 0.9075\n",
            "Epoch 4/10, Average Loss: 0.8685\n",
            "Epoch 5/10, Average Loss: 0.8437\n",
            "Epoch 6/10, Average Loss: 0.8271\n",
            "Epoch 7/10, Average Loss: 0.8153\n",
            "Epoch 8/10, Average Loss: 0.8062\n",
            "Epoch 9/10, Average Loss: 0.7990\n",
            "Epoch 10/10, Average Loss: 0.7930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=500)\n",
        "print(decode(generated[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9f8hgcQFAxu",
        "outputId": "cf9924f2-ab50-4250-8b88-7e066685361e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ROMEO:\n",
            "Come, because that a stubbary? Ratchy not the self;\n",
            "For as waving and chase notes the house,\n",
            "Nor all the northern stars like a lanen\n",
            "To take treap up the clock breaks;\n",
            "And brought deliver wrong not I draw.\n",
            "\n",
            "PETRUCHIO:\n",
            "How do you herd? then my father, good word with this young Rutland?\n",
            "There shall were my welcome with a well allegiance:\n",
            "So merning good here stands my looks.\n",
            "\n",
            "KING HENRY VI:\n",
            "I'll have thee still store thou shalt be bled by mine\n",
            "It is, that nor reasons would callow to thee a\n"
          ]
        }
      ]
    }
  ]
}